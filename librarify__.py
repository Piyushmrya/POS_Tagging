# -*- coding: utf-8 -*-
"""librarify.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tprf4FeTaYDKQVe3IJ4dSEkrNEYpFmhD
"""
import streamlit as st
import nltk
from nltk.corpus import brown
import numpy as np
import string
import regex as re
st.title(" POS Tagging ")
st.write(" Give your Input")
def split_string_with_punctuation(input_string):
    # Use regular expression to split the string
    pattern = r'(\w+|[^\w\s])'
    words_and_punctuation = re.findall(pattern, input_string)
    return words_and_punctuation
class viterbi():
    def __init__(self) -> None:
        self.all_tags = None
        self.all_words = None
        self.lexical_prob = None
        self.transition_prob = None
        self.unknown_prob = None
        self.wtoi = None
        self.ttoi = None
        self.itow = None
        self.itot = None
        self.possible_tags = {}
    def train(self, tagged_sentences):
      tagged_words = []
      for sentence in tagged_sentences:
          for word, tag in sentence:
              tagged_words.append((word, tag))
      self.all_tags = set([tag for word, tag in tagged_words])
      self.all_tags.add('^')
      self.all_words = set([word for word, tag in tagged_words])
      self.all_words.add('^')

      self.lexical_prob = np.zeros((len(self.all_tags), len(self.all_words)))
      self.transition_prob = np.zeros((len(self.all_tags), len(self.all_tags)))
      self.unknown_prob = np.zeros((len(self.all_tags), len(self.all_tags), len(self.all_tags)))

      self.wtoi = {word: i for i, word in enumerate(self.all_words)}
      self.ttoi = {tag: i for i, tag in enumerate(self.all_tags)}
      self.itow = {i: word for i, word in enumerate(self.all_words)}
      self.itot = {i: tag for i, tag in enumerate(self.all_tags)}

      sentences = list(tagged_sentences)

      for i in range(len(sentences)):
          sentences[i].insert(0, ('^', '^'))
          sentences[i].append(('^', '^'))
      # takes care of lexical probabilities
      for sentence in sentences:
          for i in range(len(sentence)):
              wordix = self.wtoi[sentence[i][0]]
              tagix = self.ttoi[sentence[i][1]]
              self.lexical_prob[tagix][wordix] += 1
              if sentence[i][0] not in self.possible_tags:
                  self.possible_tags[sentence[i][0]] = set([sentence[i][1]])
              else:
                  self.possible_tags[sentence[i][0]].add(sentence[i][1])
      # takes care of transition probabilities
      for sentence in sentences:
          for i in range(len(sentence)-1):
              tag1ix = self.ttoi[sentence[i][1]]
              tag2ix = self.ttoi[sentence[i+1][1]]
              self.transition_prob[tag1ix][tag2ix] += 1
      self.possible_tags['^'] = set(['^'])
      # handlings unknowns
      for sentence in sentences:
        for i in range(1,len(sentence)-1):
          prevtag = sentence[i-1][1]
          currtag = sentence[i][1]
          nextag = sentence[i+1][1]
          prevtagix = self.ttoi[prevtag]
          currtagix = self.ttoi[currtag]
          nextagix = self.ttoi[nextag]
          self.unknown_prob[prevtagix][nextagix][currtagix] += 1


      # normalize matrix row-wise
      for i in range(len(self.all_tags)):
          self.lexical_prob[i] /= np.sum(self.lexical_prob[i])
          self.transition_prob[i] /= np.sum(self.transition_prob[i])
      for previx in range(len(self.all_tags)):
        for nextix in range(len(self.all_tags)):
          sumgivenprevixnextix = np.sum(self.unknown_prob[previx][nextix])
          for currix in range(len(self.all_tags)):
            self.unknown_prob[previx][nextix][currix] = self.unknown_prob[previx][nextix][currix]/sumgivenprevixnextix

    def predict_sentence(self, sentence):
      probabilities = np.zeros((len(self.all_tags), len(sentence)))
      backpointers = np.zeros((len(self.all_tags), len(sentence)))
      for i in range(len(sentence)):
        if sentence[i] not in self.all_words:
            sentence[i] = self.itow[np.argmax(self.lexical_prob[self.ttoi['X'], :])]
      probabilities[self.ttoi['^'], 0] = 1
      for i in range(1, len(sentence)):
          word = sentence[i]
          wordix = self.wtoi[word]
          possible_tags_word = self.possible_tags[word]
          prev_prob = probabilities[:, i-1].tolist()
          for tag in possible_tags_word:
            tagix = self.ttoi[tag]
            for j in range(len(prev_prob)):
                probabilityAtPositioniforTagixGivenprevtagj = prev_prob[j]*self.transition_prob[j][tagix]*self.lexical_prob[tagix][wordix]
                if probabilityAtPositioniforTagixGivenprevtagj > probabilities[tagix][i]:
                    probabilities[tagix][i] = probabilityAtPositioniforTagixGivenprevtagj
                    backpointers[tagix][i] =  j
      sequence = []
      currpostag = np.argmax(probabilities[:, -1])
      sequence.append(self.itot[currpostag])
      for i in range(len(sentence)-1, -1, -1):
          nextpostag = backpointers[currpostag, i]
          currpostag = int(nextpostag)
          sequence.append(self.itot[currpostag])
      sequence.pop()
      sequence.reverse()
      # deal with unknown
      for i in range(1, len(sequence)-1):
        if sequence[i] == 'X':
          prevtag = sequence[i-1]
          nextag = sequence[i+1]
          prevtagix = self.ttoi[prevtag]
          nextagix = self.ttoi[nextag]
          currtagix = np.argmax(self.unknown_prob[prevtagix][nextagix])
          sequence[i] = self.itot[currtagix]
      return sequence


    def predict(self, sentences):
      postags = []
      for sentence in sentences:
          sentence = sentence.split()
          sentence2 = []
          for i in range(len(sentence)):
              splitted = split_string_with_punctuation(sentence[i])
              if """'""" not in splitted and '-' not in splitted:
                  sentence2.extend(splitted)
              else:
                  sentence2.append(sentence[i])
          sentence = sentence2
          sentence = ["^"] + sentence + ["^"]

          postags.append(self.predict_sentence(sentence))
      return postags

    def evaluate(self, tagged_sentences):
      sentences = []
      postags = []
      for i in range(len(tagged_sentences)):
        sentences.append([word for word, tag in tagged_sentences[i]])
        postags.append([tag for word, tag in tagged_sentences[i]])
      predicted_postags = []
      for sentence in sentences:
        sentence = ['^']+sentence+['^']
        predicted_postags.append(self.predict_sentence(sentence)[1:-1])
        # print(predicted_postags)
      postags_flattened = []
      predicted_postags_flattened = []
      for i in range(len(predicted_postags)):

        postags_flattened.extend(postags[i])

        predicted_postags_flattened.extend(predicted_postags[i])
      return postags_flattened, predicted_postags_flattened
    # def evaluate(self, tagged_sentences):









nltk.download('brown')
nltk.download('universal_tagset')
Input = st.text_input("Input")
@st.cache_data
def train_viterbi():
    vb = viterbi()
    vb.train(brown.tagged_sents(tagset='universal'))
    return vb

# Call the training function (it will be cached and run once only)
vb = train_viterbi()
output = vb.predict([Input])
st.write(output)

# vb.predict(["my train eats grass as it runs because i have made it such that it does that "])

# # perform KFOld cross validation
# from sklearn.model_selection import KFold
# kf = KFold(n_splits=5)
# sentences = brown.tagged_sents(tagset='universal')
# kf.get_n_splits(sentences)
# accuracy = 0
# for train_index, test_index in kf.split(sentences):
#     train_sentences = [sentences[i] for i in train_index]
#     test_sentences = [sentences[i] for i in test_index]
#     print('__')
#     vb = viterbi()
#     vb.train(train_sentences)
#     print('__')
#     vb.evaluate(test_sentences)
# print(accuracy/5)

# # vb = viterbi()
# # vb.train(brown.tagged_sents())

# vb.evaluate(brown.tagged_sents(tagset='universal')[:5])

# brown.tagged_sents(tagset='universal')[1]

